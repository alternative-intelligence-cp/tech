# Semantic Archive

**Autonomous semantic knowledge graph and agentic ingestion pipeline** for managing technical documentation, research reports, and codebase meta-files.

## Features

- ü§ñ **Autonomous Processing**: LLM-powered classification, validation, and relationship extraction
- üîç **Hybrid Search**: Combines vector similarity (semantic) with BM25 (lexical) using Reciprocal Rank Fusion
- üìä **Knowledge Graph**: PostgreSQL-based semantic graph with cycle detection
- üóúÔ∏è **Auto-Archive**: Compresses and removes processed files to keep repository clean
- ‚ö° **Async Pipeline**: BullMQ-based queue prevents blocking, handles retries gracefully
- üéØ **Codebase-Aware**: Custom text search dictionary preserves exact technical terminology

## Architecture

```
File ‚Üí Watcher ‚Üí Queue ‚Üí [Extractor ‚Üí Classifier ‚Üí Checker ‚Üí Function Caller ‚Üí Embedder] ‚Üí Database ‚Üí Archive
```

**Models Used:**
- **Classifier & Checker**: `nemotron-3-nano:30b` (reasoning, validation)
- **Function Caller**: `qwen3-coder:30b` (structured output generation)
- **Embedder**: `qwen3-embedding:8b` (semantic vectors)

## Prerequisites

**System Requirements:**
- Node.js 18+
- PostgreSQL 14+ (with pgvector extension)
- Redis 6+
- Ollama with models: `nemotron-3-nano:30b`, `qwen3-coder:30b`, `qwen3-embedding:8b`

**System Binaries:**
```bash
# Ubuntu/Debian
sudo apt install poppler-utils pandoc

# macOS
brew install poppler pandoc
```

## Installation

```bash
# Clone to your tech repo
cd /home/randy/Workspace/REPOS/tech
git clone <this-repo> semantic-archive
cd semantic-archive

# Install dependencies
npm install

# Configure (optional - edit config.js for custom settings)
cp config.js config.local.js

# Initialize database
npm run init-db
```

## Configuration

Edit `config.js` or set environment variables:

```bash
export POSTGRES_DB=semantic_archive
export POSTGRES_USER=your_user
export INGEST_PATH=/path/to/watch
export ARCHIVE_PATH=/path/to/archive.zip
```

## Usage

### 1. Start the Pipeline

Watch a directory for new files:

```bash
npm start watch
```

Drop PDF, DOCX, TXT, or MD files into `./ingest` and they'll be automatically processed.

### 2. Process Single File

```bash
npm start process /path/to/document.pdf
```

### 3. Search the Database

Search by natural language:

```bash
npm run search "Redis connection pooling errors"
```

Find exact technical terms:

```bash
npm run search "ERR_TIMEOUT_99"
```

Show relationships:

```bash
node search.js relationships <entity-id>
```

## How It Works

### Document Processing Pipeline

1. **Extraction**: Uses `pdftotext` (with `-layout` flag) or `pandoc` to extract text while preserving structure
2. **Classification**: Nemotron analyzes content and extracts:
   - Title, document type, summary
   - Key entities (technologies, concepts, people)
3. **Validation**: Checker agent verifies classification accuracy
4. **Function Calling**: Qwen3-coder generates database insertion commands
5. **Embedding**: Creates 3584-dim vector for semantic search
6. **Database**: Executes all inserts in single transaction (rollback on failure)
7. **Archive**: Compresses file to ZIP, deletes original

### Hybrid Search Algorithm

Combines two search methods:

**Vector Search** (Semantic Understanding):
- Finds conceptually similar documents
- "performance issues" matches "latency problems"

**Lexical Search** (Exact Matching):
- Finds specific terms and codes
- `ERR_TIMEOUT_99` matches exactly
- Custom dictionary preserves technical jargon

**Reciprocal Rank Fusion (RRF)**:
- Merges results by ranking position, not raw scores
- Documents appearing in *both* result sets bubble to top

Formula: `RRF_score(d) = Œ£(1 / (60 + rank_i))` for each ranker `i`

### Knowledge Graph Schema

**Entities** (nodes):
- Documents (your files)
- Concepts (extracted entities)

**Relationships** (edges):
- `MENTIONS`: Document ‚Üí Concept
- Custom types generated by LLM

**Cycle Prevention**: Recursive CTE checks for loops before insertion

## Example Workflow

```bash
# 1. Initialize
npm run init-db

# 2. Start watcher
npm start watch

# 3. In another terminal, copy files to ingest/
cp ~/research-reports/*.pdf ./ingest/

# 4. Watch logs as files are processed
# (extraction ‚Üí classification ‚Üí validation ‚Üí database ‚Üí archive)

# 5. Search when ready
npm run search "consciousness wave propagation"
```

**Output:**
```
--- Found 3 results for: "consciousness wave propagation" ---

1. Nikola Consciousness Field Integration (Score: 0.0724, Match: both)
   Type: TechSpec
   File: NIKOLA_ARIA.md
   Entities: ConsciousnessField, wave_propagation, FFI, Redis...
   Summary: Technical specification for integrating the
   ConsciousnessField C++ library with Aria's 6-stream I/O...
```

## Database Schema

```sql
entity (
  id text PRIMARY KEY,
  _type text,                -- Document type or entity type
  _class text,               -- 'Document' or 'Concept'
  content text,              -- Summary or full text
  embedding vector(3584),    -- Semantic vector
  metadata jsonb,            -- Flexible properties
  search_vector tsvector     -- Generated for full-text search
)

relationship (
  source_entity_id text,
  target_entity_id text,
  _class text,               -- Relationship type (MENTIONS, REFERENCES, etc.)
  metadata jsonb,
  CONSTRAINT check_no_cycles CHECK (NOT has_cycle(...))
)
```

## Troubleshooting

**"pdftotext: command not found"**
```bash
sudo apt install poppler-utils  # Ubuntu/Debian
brew install poppler            # macOS
```

**"Ollama connection refused"**
```bash
ollama serve  # Start Ollama server
ollama pull nemotron-3-nano:30b
ollama pull qwen3-coder:30b
ollama pull qwen3-embedding:8b
```

**"Redis connection failed"**
```bash
redis-server  # Start Redis
# Or: sudo systemctl start redis
```

**"pgvector extension not found"**
```sql
-- Install pgvector first
-- Ubuntu: sudo apt install postgresql-16-pgvector
-- Then in psql:
CREATE EXTENSION vector;
```

## Performance Tuning

**Adjust concurrency** (config.js or index.js):
```javascript
concurrency: 2  // Process N documents simultaneously
```

**Adjust rate limits**:
```javascript
limiter: {
  max: 10,
  duration: 60000  // Max 10 jobs per minute
}
```

**Disable validation** for faster processing:
```javascript
checkValidation: false
```

## Project Structure

```
semantic-archive/
‚îú‚îÄ‚îÄ config.js           # Configuration
‚îú‚îÄ‚îÄ init-db.js          # Database schema setup
‚îú‚îÄ‚îÄ extractor.js        # PDF/DOCX text extraction
‚îú‚îÄ‚îÄ agents.js           # LLM pipeline (Classifier, Checker, Function Caller)
‚îú‚îÄ‚îÄ executor.js         # Database command execution
‚îú‚îÄ‚îÄ archiver.js         # ZIP compression
‚îú‚îÄ‚îÄ index.js            # Main BullMQ worker + file watcher
‚îú‚îÄ‚îÄ search.js           # Hybrid search CLI
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ README.md
```

## License

MIT

## Credits

Architecture based on:
- [Hybrid Search in PostgreSQL: The Missing Manual - ParadeDB](https://www.paradedb.com/blog/hybrid-search-in-postgresql-the-missing-manual)
- [Function Calling in Agentic Workflows - Neo4j](https://medium.com/neo4j/function-calling-in-agentic-workflows-92ff33be0975)
- BullMQ documentation: https://docs.bullmq.io/
